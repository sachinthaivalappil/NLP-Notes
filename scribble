Basic text preprocessing

step 1: cleaning data

Tokenization-->stop words--> stemming-->lemmatization

Tokenization means coverting sentence into words

removing stop words(words that are not importent)

stemming - process of reducing words to its base stemming and extract rootword

lemmatization - process of remapping words to its base stemmed stem words

stemming can be used in spam classification, review classification

lemmatization can be used for text classification, language transilation, chatbot

step 2: word to vector 

text processing --> words-->vectors

1. one hot encoding 
2. bag of words 
3. TF-IDF
4. word to vector

basic terminologies used in NLP
1. corpus-paragraphs
2. document-sentences
3. vocabulary- unique words
4. words-words


one hot encoding
advantages:
very simple to implement
intuitive

disadvantages:
sparse matrix
out of vocabulary : can not handle newdata
not fixed size: size vector for each document vary
semantic meaning between words is not captured


Bag of words
creating vector based on the frequency of the words in a document

advantages
simple and intuitive

disvantages
sparsity
out of vocbulary
ordering of the words changed
not able to capture symantic meaning

Ngrams(bigram,trigram)






 

